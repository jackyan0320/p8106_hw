---
title: "P8106_hw4_xy2395"
author: "Jack Yan"
date: "4/16/2019"
output: github_document
---

```{r setup, include=TRUE, message=F, warning=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # data manipulation
library(lasso2) # data Problem 1
library(ISLR) # data Problem 2
library(rpart) # CART
library(rpart.plot)
library(caret) # model tuning
library(ranger) # much faster than RandomForest
library(patchwork)

# parallel processing with caret
library(doParallel)
cluster <- makePSOCKcluster(5)
registerDoParallel(cluster)
```

# Problem 1
### (a)

Fit a regression tree with lpsa as the response and the other variables as predictors.
```{r, message=F}
set.seed(1)
data(Prostate)
tree_rpart = rpart(lpsa~., data = Prostate, cp = 0) 
rpart.plot(tree_rpart)
cpTable <- printcp(tree_rpart) 
plotcp(tree_rpart)
minErr <- which.min(cpTable[,4]); minErr
one_se = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5],1][1]
```

Tree size 8 corresponds to the lowest cross-validation error. The tree size obtained by 1SE rule is 4, so they are not the same.

```{r, include=F, eval=F}
# use caret to do cross validation
set.seed(1)
ctrl <- trainControl(method = "cv")
rpart2.fit <- train(lpsa~., Prostate, 
                   method = "rpart2",
                   tuneGrid = data.frame(maxdepth = 0:10),
                   trControl = ctrl)
ggplot(rpart2.fit, highlight = TRUE)
rpart.plot(rpart2.fit$finalModel)
rpart2.fit$bestTune
```

### (b)

Create a plot of the final tree.
```{r}
# use 1SE rule
tree_pruned = prune(tree_rpart, cp = one_se)
rpart.plot(tree_pruned)
```

1 SE rule was used to prune the regression tree. We got a tree with size 4. The interpretation of the rightmost terminal node is that for observations with `lcavol` (log cancer volume) >= 2.5, the value of response (log prostate specific antigen) is predicted to be 3.8.

### (c) Bagging
```{r}
# For bagging, don't have to tune
# fit the bagging model
bagging <- ranger(lpsa~., Prostate,
                  mtry = 8,
                  importance = "permutation",
                  min.node.size = 25,
                  scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(bagging), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

### (d) Random Forest
```{r}
set.seed(1)
ctrl <- trainControl(method = "cv")
# Tune the Random Forest model
rf.grid <- expand.grid(mtry = 1:8, 
                       splitrule = "variance",
                       min.node.size = 1:30)
rf.fit <- train(lpsa~., Prostate,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)

rf.fit$bestTune
ggplot(rf.fit, highlight = TRUE)
# Fit a random forest model using the best tuning parameters
rf <- ranger(lpsa~., Prostate,
             mtry = 4,
             min.node.size = 18,
             importance = "permutation",
             scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(rf), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

### (e) Boosting
```{r}
set.seed(1)
# Tune the boosting model
gbm.grid <- expand.grid(n.trees = c(2000,2250,2500,2750,3000),
                        interaction.depth = 1:3,
                        shrinkage = c(0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045),
                        n.minobsinnode = 1)

gbm.fit <- train(lpsa~., Prostate,
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm.fit, highlight = TRUE)
gbm.fit$bestTune

# variable importance
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

### (e) Comparing models
```{r}
resamp <- resamples(list(rf = rf.fit, gbm = gbm.fit))
summary(resamp)
ggplot(resamp) + theme_bw()
```
Bagging is a special case of random forest in which mtry = total number of predictors = 8. According to the tuning of random forest, the best mtry = 4, so apparently random forest is better than bagging.

Here we compare the cross-validation RMSE of random forest and boosting. I will choose the boosting model to predict PSA level, because boosting has lower cross-validation RMSE than that of random forest, indicating the best prediction accuracy. 



# Problem 2

Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. Use set.seed() for reproducible results.
```{r, message=FALSE}
data(OJ)

oj <-
  as.tibble(OJ) %>% 
  mutate(Store7 = recode(Store7, '1' = 'Yes', '2' = 'No'),
         Store7 = as.numeric(Store7))

#split the data into training and test sets
set.seed(1)
rowTrain <- createDataPartition(y = oj$Purchase,
                                p = 799/1070,
                                list = FALSE)
train_df = oj[rowTrain,]
test_df = oj[-rowTrain,]
```

### (a) Classification Tree
```{r, eval=F}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
rpart.fit <- train(Purchase~., train_df, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-20,-2, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
```

```{r, echo=F,include=F}
# saveRDS(rpart.fit, 'hw4_rpart_fit.rds')
rpart.fit = readRDS('hw4_rpart_fit.rds')
```

```{r }
rpart.fit$bestTune
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

```{r}
test_df$probCH = predict(rpart.fit$finalModel, newdata = test_df, type = "prob")[,1]
test_df$pred = if_else(test_df$probCH > 0.5, 'CH', 'MM')

# Classification error rate
1 - mean(test_df$pred == test_df$Purchase)
```

Test classification error rate is `r 1 - mean(test_df$pred == test_df$Purchase)`.


### (b) Random Forests
```{r, eval = F}
rf.grid <- expand.grid(mtry = seq(4,12, by=1),
                       splitrule = "gini",
                       min.node.size = seq(20,55, by=3))
set.seed(1)
rf.fit <- train(Purchase~., train_df,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
```

```{r, echo=F}
# saveRDS(rf.fit, 'hw4_rf_fit.rds')
rf.fit = readRDS('hw4_rf_fit.rds')
```

```{r }
rf.fit$bestTune
ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = test_df, type = "prob")[,1]
pred = if_else(rf.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(pred == test_df$Purchase)
```

##### Variable Importance
```{r}
set.seed(1)

rf2.final.per <- ranger(Purchase~., train_df, 
                        mtry = 9, 
                        min.node.size = 41,
                        splitrule = "gini",
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))

set.seed(1)
rf2.final.imp <- ranger(Purchase~., train_df, 
                        mtry = 9, 
                        splitrule = "gini",
                        min.node.size = 41,
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))

```

### (b) Boosting
#### Binomial loss
```{r, eval=F}
gbmB.grid <- expand.grid(n.trees = c(500,1000,1500,2000),
                        interaction.depth = 2:5,
                        shrinkage = c(0.001,0.002,0.003,0.004),
                        n.minobsinnode = 1)
set.seed(1)
# Binomial loss function
gbmB.fit <- train(Purchase~., train_df, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)
```

```{r, echo=F,include=F}
# saveRDS(gbmB.fit, 'hw4_gbmB_fit.rds')
gbmB.fit = readRDS('hw4_gbmB_fit.rds')
```

```{r}
ggplot(gbmB.fit, highlight = TRUE)
gbmB.pred <- predict(gbmB.fit, newdata = test_df, type = "prob")[,1]
class_gbmB = if_else(gbmB.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(class_gbmB == test_df$Purchase)
```

#### AdaBoost
```{r, eval=F}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000,5000),
                        interaction.depth = 2:7,
                        shrinkage = c(0.0001,0.0005,0.001,0.002),
                        n.minobsinnode = 1)
set.seed(1)
# Adaboost loss function
gbmA.fit <- train(Purchase~., train_df,
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
```

```{r, echo=F}
# saveRDS(gbmB.fit, 'hw4_gbmA_fit.rds')
gbmA.fit = readRDS('hw4_gbmA_fit.rds')
```

```{r}
ggplot(gbmA.fit, highlight = TRUE)
gbmA.pred <- predict(gbmA.fit, newdata = test_df, type = "prob")[,1]
class_gbmA = if_else(gbmA.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(class_gbmA == test_df$Purchase)
```

##### Variable Importance
```{r}
summary(gbmB.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```


```{r, echo=F}
stopCluster(cluster)
```

