---
title: "P8106_hw4_xy2395"
author: "Jack Yan"
date: "4/16/2019"
output: github_document
---

```{r setup, include=TRUE, message=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # data manipulation
library(lasso2) # data Problem 1
library(ISLR) # data Problem 2
library(rpart) # CART
library(rpart.plot)
library(caret)
library(ranger) # much faster
```

# Problem 1
### (a)

Fit a regression tree with lpsa as the response and the other variables as predictors.
```{r, message=F}
set.seed(1)
data(Prostate)
tree_rpart = rpart(lpsa~., data = Prostate, cp = 0) 
rpart.plot(tree_rpart)
cpTable <- printcp(tree_rpart) 
plotcp(tree_rpart)
minErr <- which.min(cpTable[,4]); minErr
one_se = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5],1][1]
```

Tree size 8 corresponds to the lowest cross-validation error. The tree size obtained by 1SE rule is 4, so they are not the same.

```{r, include=F, eval=F}
# use caret to do cross validation
set.seed(1)
ctrl <- trainControl(method = "cv")
rpart2.fit <- train(lpsa~., Prostate, 
                   method = "rpart2",
                   tuneGrid = data.frame(maxdepth = 0:10),
                   trControl = ctrl)
ggplot(rpart2.fit, highlight = TRUE)
rpart.plot(rpart2.fit$finalModel)
rpart2.fit$bestTune
```

### (b)

Create a plot of the final tree.
```{r}
# use 1SE rule
tree_pruned = prune(tree_rpart, cp = one_se)
rpart.plot(tree_pruned)
```

1 SE rule was used to prune the regression tree. We got a tree with size 4. The interpretation of the rightmost terminal node is that for observations with `lcavol` (log cancer volume) >= 2.5, the value of response (log prostate specific antigen) is predicted to be 3.8.

### (c) Bagging
```{r}
# For bagging, don't have to tune
# fit the bagging model
bagging <- ranger(lpsa~., Prostate,
                  mtry = 8,
                  importance = "permutation",
                  min.node.size = 25,
                  scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(bagging), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

### (d) Random Forest
```{r}
set.seed(1)
ctrl <- trainControl(method = "cv")
# Tune the Random Forest model
rf.grid <- expand.grid(mtry = 1:8, 
                       splitrule = "variance",
                       min.node.size = 1:30)
rf.fit <- train(lpsa~., Prostate,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)

rf.fit$bestTune
ggplot(rf.fit, highlight = TRUE)
# Fit a random forest model using the best tuning parameters
rf <- ranger(lpsa~., Prostate,
             mtry = 4,
             min.node.size = 18,
             importance = "permutation",
             scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(rf), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

### (e) Boosting
```{r}
set.seed(1)
# Tune the boosting model
gbm.grid <- expand.grid(n.trees = c(2000,2250,2500,2750,3000),
                        interaction.depth = 1:3,
                        shrinkage = c(0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045),
                        n.minobsinnode = 1)

gbm.fit <- train(lpsa~., Prostate,
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm.fit, highlight = TRUE)
gbm.fit$bestTune

# variable importance
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

### (e) Comparing models
```{r}
resamp <- resamples(list(rf = rf.fit, gbm = gbm.fit))
summary(resamp)
ggplot(resamp) + theme_bw()
```
Bagging is a special case of random forest in which mtry = total number of predictors = 8. According to the tuning of random forest, the best mtry = 4, so apparently random forest is better than bagging.

Here we compare the cross-validation RMSE of random forest and boosting. I will choose the boosting model to predict PSA level, because boosting has lower cross-validation RMSE than that of random forest, indicating the best prediction accuracy. 



# Problem 2

Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. Use set.seed() for reproducible results.
```{r, message=FALSE}
data(OJ)
oj = as.tibble(OJ)

#split the data into training and test sets
set.seed(1)
train_df = sample_n(oj, 800) 
test_df = anti_join(oj, train_df)
```

### (a)
