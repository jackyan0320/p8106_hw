---
title: "P8106_hw4_xy2395"
author: "Jack Yan"
date: "4/16/2019"
output: html_document
---

```{r setup, include=TRUE, message=F, warning=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # data manipulation
library(lasso2) # data Problem 1
library(ISLR) # data Problem 2
library(rpart) # CART
library(rpart.plot)
library(caret) # model tuning
library(ranger) # much faster than RandomForest
library(patchwork)

# parallel processing with caret
library(doParallel)
cluster <- makePSOCKcluster(10)
registerDoParallel(cluster)
```

# Problem 1
### (a) Regression Tree

Fit a regression tree with lpsa as the response and the other variables as predictors.
```{r, message=F}
set.seed(1)
data(Prostate)
tree_rpart = rpart(lpsa~., data = Prostate, cp = 0) 
rpart.plot(tree_rpart)

# Show cross-validation error
cpTable <- printcp(tree_rpart) 

# Plot cross-validation error with different tree sizes
plotcp(tree_rpart)
minErr <- which.min(cpTable[,4]); minErr
one_se = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5],1][1]; one_se
```

The Tree size corresponding to the lowest cross-validation error is 8, while the tree size obtained by 1SE rule is 4. Therefore, the tree sizes obtained by the two rules are different. 1SE rule generally generates smaller trees.

```{r, include=F, eval=F}
# use caret to do cross validation
set.seed(1)
ctrl <- trainControl(method = "cv")
rpart2.fit <- train(lpsa~., Prostate, 
                   method = "rpart2",
                   tuneGrid = data.frame(maxdepth = 0:10),
                   trControl = ctrl)
ggplot(rpart2.fit, highlight = TRUE)
rpart.plot(rpart2.fit$finalModel)
rpart2.fit$bestTune
```

### (b) Plotting Regression Tree

Create a plot of the final tree. The 1SE rule is used.
```{r}
# use 1SE rule
tree_pruned = prune(tree_rpart, cp = one_se)
rpart.plot(tree_pruned)
```

1 SE rule was used to prune the regression tree. We got a tree with size 4. The interpretation of the bottom right terminal node is that for observations with `lcavol` (log cancer volume) >= 2.5, the value of response (log prostate specific antigen) is predicted to be 3.8.

### (c) Bagging and variable importance
```{r}
# For bagging, don't have to tune
# fit the bagging model
bagging <- ranger(lpsa~., Prostate,
                  mtry = 8,
                  importance = "permutation",
                  min.node.size = 25,
                  scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(bagging), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

For bagging, we don't have to tune the `mtry` parameter. We set `min.node.size` to be 25. The variable importance plot is shown above. `lcavol` is the most important predictor, and `lweight` is the second most important one. 

### (d) Random Forest
```{r, eval = F}
set.seed(1)
ctrl <- trainControl(method = "cv")
# Tune the Random Forest model
rf.grid <- expand.grid(mtry = 1:8, 
                       splitrule = "variance",
                       min.node.size = 1:30)
rf.fit <- train(lpsa~., Prostate,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)
```

```{r, echo = F}
# saveRDS(rf.fit, 'hw4_rf_fit.rds')
rf.fit = readRDS('hw4_rf_fit.rds')
```

```{r }
rf.fit$bestTune
ggplot(rf.fit, highlight = TRUE)
# Fit a random forest model using the best tuning parameters
rf <- ranger(lpsa~., Prostate,
             mtry = 4,
             min.node.size = 10,
             importance = "permutation",
             scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(rf), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```
The best tuning parameter is `mtry` = 4 and  `min.node.size` = 10. As shown in the plot, `lcavol` is the most important predictor. `lweight` and `svi` are 2nd and 3rd important variables.

### (e) Boosting
```{r, eval=F}
set.seed(1)
# Tune the boosting model
gbm.grid <- expand.grid(n.trees = c(2000,2250,2500,2750,3000),
                        interaction.depth = 1:3,
                        shrinkage = c(0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045),
                        n.minobsinnode = 1)

gbm.fit <- train(lpsa~., Prostate,
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)
```

```{r, echo=F}
# saveRDS(gbm.fit, 'hw4_gbm_fit.rds')
gbm.fit = readRDS('hw4_gbm_fit.rds')
```

```{r }
ggplot(gbm.fit, highlight = TRUE)
gbm.fit$bestTune

# variable importance
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```
For the boosting model, the best model has `interaction depth` = 1 and `shrinkage` = 0.003, adding up 2750 trees. `lcavol` is the most important predictor, followed by `lweight` and `lcp`.
 
### (e) Comparing models

A single regression tree has high variance and low prediction accuracy, so I would use ensemble methods instead of a single regression tree to predict PSA level. Among the ensemble methods, bagging is a special case of random forest in which mtry = total number of predictors = 8. According to the tuning process of random forest, the best mtry = 4 < 8, so apparently random forest is better than bagging. Here we compare random forest and boosting using cross validation. Boosting has lower cross-validation RMSE than that of random forest, indicating the best prediction accuracy. Therefore, I will choose the boosting model to predict PSA level.

```{r}
resamp <- resamples(list(rf = rf.fit, gbm = gbm.fit))
summary(resamp)
ggplot(resamp) + theme_bw()
```

# Problem 2

Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. Use set.seed() for reproducible results.
```{r, message=FALSE}
data(OJ)

oj <-
  as.tibble(OJ) %>% 
  mutate(Store7 = recode(Store7, '1' = 'Yes', '2' = 'No'),
         Store7 = as.numeric(Store7))

#split the data into training and test sets
set.seed(1)
rowTrain <- createDataPartition(y = oj$Purchase,
                                p = 799/1070,
                                list = FALSE)
train_df = oj[rowTrain,]
test_df = oj[-rowTrain,]
dim(train_df)
```

### (a) Classification Tree
```{r, eval=F}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
rpart.fit <- train(Purchase~., train_df, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-20,-2, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
```

```{r, echo=F,include=F}
# saveRDS(rpart.fit, 'hw4_rpart_fit.rds')
rpart.fit = readRDS('hw4_rpart_fit.rds')
```

```{r }
rpart.fit$bestTune
# Model tuning
ggplot(rpart.fit, highlight = TRUE)
# Plot of the final model
rpart.plot(rpart.fit$finalModel)
```

The plot of the final model is shown above.

```{r}
test_df$probCH = predict(rpart.fit$finalModel, newdata = test_df, type = "prob")[,1]
test_df$pred = if_else(test_df$probCH > 0.5, 'CH', 'MM')

# Classification error rate
1 - mean(test_df$pred == test_df$Purchase)
```

Test classification error rate of classification tree is `r 1 - mean(test_df$pred == test_df$Purchase)`.

### (b) Random Forests
```{r, eval = F}
rf.grid <- expand.grid(mtry = seq(4,12, by=1),
                       splitrule = "gini",
                       min.node.size = seq(20,55, by=3))
set.seed(1)
rf.fit <- train(Purchase~., train_df,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
```

```{r, echo=F}
# saveRDS(rf.fit, 'hw4p2_rf_fit.rds')
rf.fit = readRDS('hw4p2_rf_fit.rds')
```

```{r }
rf.fit$bestTune
ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = test_df, type = "prob")[,1]
pred_rf = if_else(rf.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(pred_rf == test_df$Purchase)
```

The test classification error rate for random forest is `r 1 - mean(pred_rf == test_df$Purchase)`.

##### Variable Importance
```{r}
set.seed(1)

rf2.final.per <- ranger(Purchase~., train_df, 
                        mtry = 9, 
                        min.node.size = 41,
                        splitrule = "gini",
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))

set.seed(1)
rf2.final.imp <- ranger(Purchase~., train_df, 
                        mtry = 9, 
                        splitrule = "gini",
                        min.node.size = 41,
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))
```

Variable importance is measured based on permutation and node impurity. Based on both criteria, `LoyalCH` is the most important variable, followed by `PriceDiff`. However, the rankings of other important variables are different.

### (b) Boosting
#### Binomial loss
```{r, eval=F}
gbmB.grid <- expand.grid(n.trees = c(1000,2000,3000,4000,5000,6000),
                        interaction.depth = 2:8,
                        shrinkage = c(0.00025,0.0005,0.001,0.002,0.003,0.004),
                        n.minobsinnode = 1)
set.seed(1)
# Binomial loss function
gbmB.fit <- train(Purchase~., train_df, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)
```

```{r, echo=F,include=F}
# saveRDS(gbmB.fit, 'hw4_gbmB_fit.rds')
gbmB.fit = readRDS('hw4_gbmB_fit.rds')
```

```{r}
ggplot(gbmB.fit, highlight = TRUE)
gbmB.pred <- predict(gbmB.fit, newdata = test_df, type = "prob")[,1]
class_gbmB = if_else(gbmB.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(class_gbmB == test_df$Purchase)
```

The test classification error rate for Boosting using binomial loss function is `r 1 - mean(class_gbmB == test_df$Purchase)`.

#### AdaBoost
```{r, eval=F}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000,5000,6000,7000,8000),
                        interaction.depth = 2:7,
                        shrinkage = c(0.0005,0.001,0.002,0.003),
                        n.minobsinnode = 1)
set.seed(1)
# Adaboost loss function
gbmA.fit <- train(Purchase~., train_df,
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
```

```{r, echo=F}
# saveRDS(gbmA.fit, 'hw4_gbmA_fit.rds')
gbmA.fit = readRDS('hw4_gbmA_fit.rds')
```

```{r}
ggplot(gbmA.fit, highlight = TRUE)
gbmA.pred <- predict(gbmA.fit, newdata = test_df, type = "prob")[,1]
class_gbmA = if_else(gbmA.pred > 0.5, 'CH', 'MM')
# Classification error rate
1 - mean(class_gbmA == test_df$Purchase)
```

The test classification error rate for AdaBoost is `r 1 - mean(class_gbmA == test_df$Purchase)`.

##### Variable Importance
```{r}
# Binomial loss function
summary(gbmB.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
# AdaBoost
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```
Variable importance plots using Binomial loss function and AdaBoost are reported above. `LoyalCH` is the most important variable in both cases. 

```{r, echo=F}
stopCluster(cluster)
```

